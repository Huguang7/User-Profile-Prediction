{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>keywords</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fibre:16;quoi:1;dangers:1;combien:1;hightech:1...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>restaurant:1;marrakech.shtml:1</td>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>payer:1;faq:1;taxe:1;habitation:1;macron:1;qui...</td>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>rigaud:3;laurent:3;photo:11;profile:8;photopro...</td>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>societe:1;disparition:1;proche:1;m%c3%a9lanie....</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           keywords  age sex\n",
       "0   1  fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62   F\n",
       "1   2                     restaurant:1;marrakech.shtml:1   35   M\n",
       "2   3  payer:1;faq:1;taxe:1;habitation:1;macron:1;qui...   45   F\n",
       "3   4  rigaud:3;laurent:3;photo:11;profile:8;photopro...   46   F\n",
       "4   5  societe:1;disparition:1;proche:1;m%c3%a9lanie....   42   F"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.read_csv('./train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10w = data.iloc[:100000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_10w.to_csv('train_10w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x109bd6fd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.sex.value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.iloc[:20000,:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             0\n",
       "keywords    2226\n",
       "age            0\n",
       "sex            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17774, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "data = data.reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['nbr_words'] = data['keywords'].apply(lambda x : x.count(\";\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>keywords</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>nbr_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fibre:16;quoi:1;dangers:1;combien:1;hightech:1...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>restaurant:1;marrakech.shtml:1</td>\n",
       "      <td>35</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>payer:1;faq:1;taxe:1;habitation:1;macron:1;qui...</td>\n",
       "      <td>45</td>\n",
       "      <td>F</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>rigaud:3;laurent:3;photo:11;profile:8;photopro...</td>\n",
       "      <td>46</td>\n",
       "      <td>F</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>societe:1;disparition:1;proche:1;m%c3%a9lanie....</td>\n",
       "      <td>42</td>\n",
       "      <td>F</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           keywords  age sex  nbr_words\n",
       "0   1  fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62   F         57\n",
       "1   2                     restaurant:1;marrakech.shtml:1   35   M          1\n",
       "2   3  payer:1;faq:1;taxe:1;habitation:1;macron:1;qui...   45   F          7\n",
       "3   4  rigaud:3;laurent:3;photo:11;profile:8;photopro...   46   F         17\n",
       "4   5  societe:1;disparition:1;proche:1;m%c3%a9lanie....   42   F         18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "droprows = data[data['nbr_words'] < 50].index\n",
    "data = data.drop(droprows).reset_index(drop = True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToDict(lst):\n",
    "    '''\n",
    "    convert a list to dictionary\n",
    "    e.g. 'restaurant:1;marrakech.shtml:1' --> {'restaurant:1, marrakech.shtml:1}\n",
    "    \n",
    "    '''\n",
    "    op = {lst[i]: lst[i + 1] for i in range(0, len(lst), 2)}\n",
    "\n",
    "    return op\n",
    "\n",
    "def real_sent(input_dict):\n",
    "    '''\n",
    "    combine the word with frequency to one element \n",
    "    '''\n",
    "    sent = ''\n",
    "    for word in input_dict:\n",
    "        sent = sent + (word +' ') * input_dict[word]\n",
    "    return sent\n",
    "\n",
    "def token(data):\n",
    "    '''\n",
    "    from word with frequency to a complete piece \n",
    "    e.g. 'restaurant:1;marrakech.shtml:1' --> restaurant marrakech.shtml\n",
    "    '''\n",
    "\n",
    "    target = []\n",
    "    for element in data:\n",
    "        # split the word frequency \n",
    "        element = element.replace(';', ':')\n",
    "        element = element.split(':')\n",
    "        \n",
    "        # skip the Indexerror \n",
    "        try:\n",
    "            the_dict = listToDict(element)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        \n",
    "        # final data\n",
    "        for key in the_dict:\n",
    "            the_dict[key] = int(the_dict[key])\n",
    "        target.append(real_sent(the_dict))\n",
    "\n",
    "    \n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i i i i i i i i i i i i i i i i fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre fibre quoi dangers combien hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech hightech que que que que que que que que que que que que que que que que est recevra operateur quelles installation monde tout oblige differences existe adsl choisir concerne sante telephonique com chez faire quelle debit les les cable quels quels fttb box ligne pour pour pour sur sur sur sur sur sur sur sur sur sur sur sur sur sur sur sur pose pose pose pose pose pose pose pose pose pose pose pose pose pose pose pose devient meilleur fttla suis suis suis des linternaute coute avec ftth avoir qui qui avant effectue libre debits celui delais internet internet internet internet internet internet internet internet internet internet internet internet internet internet internet internet abonner questions questions questions questions questions questions questions questions questions questions questions questions questions questions questions questions toujours sont sont mon ',\n",
       " 'restaurant marrakech.shtml ',\n",
       " 'payer faq taxe habitation macron qui detail programme ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test piece of data to have a look\n",
    "test = ['i:16;fibre:16;quoi:1;dangers:1;combien:1;hightech:16;que:16;est:1;recevra:1;operateur:1;quelles:1;installation:1;monde:1;tout:1;oblige:1;differences:1;existe:1;adsl:1;choisir:1;concerne:1;sante:1;telephonique:1;com:1;chez:1;faire:1;quelle:1;debit:1;les:2;cable:1;quels:2;fttb:1;box:1;ligne:1;pour:3;sur:16;pose:16;devient:1;meilleur:1;fttla:1;suis:3;des:1;linternaute:1;coute:1;avec:1;ftth:1;avoir:1;qui:2;avant:1;effectue:1;libre:1;debits:1;celui:1;delais:1;internet:16;abonner:1;questions:16;toujours:1;sont:2;mon:1',\n",
    "        'restaurant:1;marrakech.shtml:1','payer:1;faq:1;taxe:1;habitation:1;macron:1;qui:1;detail:1;programme:1']\n",
    "token(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>keywords</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>nbr_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fibre:16;quoi:1;dangers:1;combien:1;hightech:1...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>fibre fibre fibre fibre fibre fibre fibre fibr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>68</td>\n",
       "      <td>tabac les terrorisme excuses excuses luxe luxe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>111</td>\n",
       "      <td>lively messi enzo nolwenn diaz kourtney beckha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>230</td>\n",
       "      <td>adams refaeli paltrow brodier brodier ariane a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>service:1;siege:1;avenue:1;douleur+a+la+poitri...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>155</td>\n",
       "      <td>service siege avenue douleur+a+la+poitrine+cot...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           keywords  age sex  nbr_words  \\\n",
       "0   1  fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62   F         57   \n",
       "1  24  tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...   40   M         68   \n",
       "2  28  lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...   50   F        111   \n",
       "3  33  adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...   52   F        230   \n",
       "4  42  service:1;siege:1;avenue:1;douleur+a+la+poitri...   62   F        155   \n",
       "\n",
       "                                                text  \n",
       "0  fibre fibre fibre fibre fibre fibre fibre fibr...  \n",
       "1  tabac les terrorisme excuses excuses luxe luxe...  \n",
       "2  lively messi enzo nolwenn diaz kourtney beckha...  \n",
       "3  adams refaeli paltrow brodier brodier ariane a...  \n",
       "4  service siege avenue douleur+a+la+poitrine+cot...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = token(data.keywords)\n",
    "data['text'] = pd.DataFrame(tokens)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stop.french.stops import STOPS_LIST as FRENCH_STOPS\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('french') + stopwords.words('english')\n",
    "\n",
    "# remove stop words\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "data.text = data.text.astype('str')\n",
    "data.text = data.text.str.lower()\n",
    "data['text_clean'] = data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>keywords</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>nbr_words</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fibre:16;quoi:1;dangers:1;combien:1;hightech:1...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>fibre fibre fibre fibre fibre fibre fibre fibr...</td>\n",
       "      <td>fibre fibre fibre fibre fibre fibre fibre fibr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>68</td>\n",
       "      <td>tabac les terrorisme excuses excuses luxe luxe...</td>\n",
       "      <td>tabac terrorisme excuses excuses luxe luxe dol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>111</td>\n",
       "      <td>lively messi enzo nolwenn diaz kourtney beckha...</td>\n",
       "      <td>lively messi enzo nolwenn diaz kourtney beckha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>230</td>\n",
       "      <td>adams refaeli paltrow brodier brodier ariane a...</td>\n",
       "      <td>adams refaeli paltrow brodier brodier ariane a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>service:1;siege:1;avenue:1;douleur+a+la+poitri...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>155</td>\n",
       "      <td>service siege avenue douleur+a+la+poitrine+cot...</td>\n",
       "      <td>service siege avenue douleur a la poitrine cot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>46</td>\n",
       "      <td>lourdes:13;qui:13;pratique:13;trinitaine:2;pie...</td>\n",
       "      <td>74</td>\n",
       "      <td>F</td>\n",
       "      <td>79</td>\n",
       "      <td>lourdes lourdes lourdes lourdes lourdes lourde...</td>\n",
       "      <td>lourdes lourdes lourdes lourdes lourdes lourde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>58</td>\n",
       "      <td>sociaux:2;son:1;chateauneuf:1;petite:1;chasse:...</td>\n",
       "      <td>35</td>\n",
       "      <td>F</td>\n",
       "      <td>103</td>\n",
       "      <td>sociaux sociaux son chateauneuf petite chasse ...</td>\n",
       "      <td>sociaux sociaux chateauneuf petite chasse chim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>67</td>\n",
       "      <td>melania:1;kunakey:2;anderson:1;people:95;salue...</td>\n",
       "      <td>64</td>\n",
       "      <td>F</td>\n",
       "      <td>169</td>\n",
       "      <td>melania kunakey kunakey anderson people people...</td>\n",
       "      <td>melania kunakey kunakey anderson people people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>74</td>\n",
       "      <td>folle:1;actu:2;celebrites:9;par:1;actualites:3...</td>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>69</td>\n",
       "      <td>folle actu actu celebrites celebrites celebrit...</td>\n",
       "      <td>folle actu actu celebrites celebrites celebrit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75</td>\n",
       "      <td>faute:1;expressions:1;les:2;bougival_09378fe2:...</td>\n",
       "      <td>54</td>\n",
       "      <td>M</td>\n",
       "      <td>76</td>\n",
       "      <td>faute expressions les les bougival_09378fe2 tr...</td>\n",
       "      <td>faute expressions  travail actualite kev pisci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           keywords  age sex  nbr_words  \\\n",
       "0   1  fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62   F         57   \n",
       "1  24  tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...   40   M         68   \n",
       "2  28  lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...   50   F        111   \n",
       "3  33  adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...   52   F        230   \n",
       "4  42  service:1;siege:1;avenue:1;douleur+a+la+poitri...   62   F        155   \n",
       "5  46  lourdes:13;qui:13;pratique:13;trinitaine:2;pie...   74   F         79   \n",
       "6  58  sociaux:2;son:1;chateauneuf:1;petite:1;chasse:...   35   F        103   \n",
       "7  67  melania:1;kunakey:2;anderson:1;people:95;salue...   64   F        169   \n",
       "8  74  folle:1;actu:2;celebrites:9;par:1;actualites:3...   47   M         69   \n",
       "9  75  faute:1;expressions:1;les:2;bougival_09378fe2:...   54   M         76   \n",
       "\n",
       "                                                text  \\\n",
       "0  fibre fibre fibre fibre fibre fibre fibre fibr...   \n",
       "1  tabac les terrorisme excuses excuses luxe luxe...   \n",
       "2  lively messi enzo nolwenn diaz kourtney beckha...   \n",
       "3  adams refaeli paltrow brodier brodier ariane a...   \n",
       "4  service siege avenue douleur+a+la+poitrine+cot...   \n",
       "5  lourdes lourdes lourdes lourdes lourdes lourde...   \n",
       "6  sociaux sociaux son chateauneuf petite chasse ...   \n",
       "7  melania kunakey kunakey anderson people people...   \n",
       "8  folle actu actu celebrites celebrites celebrit...   \n",
       "9  faute expressions les les bougival_09378fe2 tr...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  fibre fibre fibre fibre fibre fibre fibre fibr...  \n",
       "1  tabac terrorisme excuses excuses luxe luxe dol...  \n",
       "2  lively messi enzo nolwenn diaz kourtney beckha...  \n",
       "3  adams refaeli paltrow brodier brodier ariane a...  \n",
       "4  service siege avenue douleur a la poitrine cot...  \n",
       "5  lourdes lourdes lourdes lourdes lourdes lourde...  \n",
       "6  sociaux sociaux chateauneuf petite chasse chim...  \n",
       "7  melania kunakey kunakey anderson people people...  \n",
       "8  folle actu actu celebrites celebrites celebrit...  \n",
       "9  faute expressions  travail actualite kev pisci...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove symbols\n",
    "import regex as re \n",
    "\n",
    "# y = pd.DataFrame(['shtml restaurant m%c3',' 444 marak 0ab','an'], columns=['col'])\n",
    "# y.str.re.sub(r'\\w*enc\\w*', '', y)\n",
    "# result: ['restaurant',' marak ','an']\n",
    "\n",
    "def clean_symbol(data,col): \n",
    "    data[col] = data[col].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    data[col] = data[col].apply(lambda x:re.sub(r'\\w*html\\w*', '', x) )\n",
    "    data[col] = data[col].apply(lambda x:re.sub(r'\\w*%\\w*', '', x) )\n",
    "    data[col] = data[col].apply(lambda x:re.sub(r'\\w*[0-9]\\w*', '', x) )\n",
    "#     data[col] = data[col].str.lower()\n",
    "    return data\n",
    "\n",
    "data = clean_symbol(data, \"text_clean\")\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>keywords</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>nbr_words</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>fibre:16;quoi:1;dangers:1;combien:1;hightech:1...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>57</td>\n",
       "      <td>fibre fibre fibre fibre fibre fibre fibre fibr...</td>\n",
       "      <td>fibre fibre fibre fibre fibre fibre fibre fibr...</td>\n",
       "      <td>[fibre, fibre, fibre, fibre, fibre, fibre, fib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...</td>\n",
       "      <td>40</td>\n",
       "      <td>M</td>\n",
       "      <td>68</td>\n",
       "      <td>tabac les terrorisme excuses excuses luxe luxe...</td>\n",
       "      <td>tabac terrorisme excuses excuses luxe luxe dol...</td>\n",
       "      <td>[tabac, terrorisme, excuses, excuses, luxe, lu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...</td>\n",
       "      <td>50</td>\n",
       "      <td>F</td>\n",
       "      <td>111</td>\n",
       "      <td>lively messi enzo nolwenn diaz kourtney beckha...</td>\n",
       "      <td>lively messi enzo nolwenn diaz kourtney beckha...</td>\n",
       "      <td>[lively, messi, enzo, nolwenn, diaz, kourtney,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...</td>\n",
       "      <td>52</td>\n",
       "      <td>F</td>\n",
       "      <td>230</td>\n",
       "      <td>adams refaeli paltrow brodier brodier ariane a...</td>\n",
       "      <td>adams refaeli paltrow brodier brodier ariane a...</td>\n",
       "      <td>[adams, refaeli, paltrow, brodier, brodier, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>service:1;siege:1;avenue:1;douleur+a+la+poitri...</td>\n",
       "      <td>62</td>\n",
       "      <td>F</td>\n",
       "      <td>155</td>\n",
       "      <td>service siege avenue douleur+a+la+poitrine+cot...</td>\n",
       "      <td>service siege avenue douleur a la poitrine cot...</td>\n",
       "      <td>[service, siege, avenue, douleur, a, la, poitr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                           keywords  age sex  nbr_words  \\\n",
       "0   1  fibre:16;quoi:1;dangers:1;combien:1;hightech:1...   62   F         57   \n",
       "1  24  tabac:1;les:1;terrorisme:1;excuses:2;luxe:2;do...   40   M         68   \n",
       "2  28  lively:1;messi:1;enzo:1;nolwenn:1;diaz:1;kourt...   50   F        111   \n",
       "3  33  adams:1;refaeli:1;paltrow:1;brodier:2;ariane:2...   52   F        230   \n",
       "4  42  service:1;siege:1;avenue:1;douleur+a+la+poitri...   62   F        155   \n",
       "\n",
       "                                                text  \\\n",
       "0  fibre fibre fibre fibre fibre fibre fibre fibr...   \n",
       "1  tabac les terrorisme excuses excuses luxe luxe...   \n",
       "2  lively messi enzo nolwenn diaz kourtney beckha...   \n",
       "3  adams refaeli paltrow brodier brodier ariane a...   \n",
       "4  service siege avenue douleur+a+la+poitrine+cot...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  fibre fibre fibre fibre fibre fibre fibre fibr...   \n",
       "1  tabac terrorisme excuses excuses luxe luxe dol...   \n",
       "2  lively messi enzo nolwenn diaz kourtney beckha...   \n",
       "3  adams refaeli paltrow brodier brodier ariane a...   \n",
       "4  service siege avenue douleur a la poitrine cot...   \n",
       "\n",
       "                                               token  \n",
       "0  [fibre, fibre, fibre, fibre, fibre, fibre, fib...  \n",
       "1  [tabac, terrorisme, excuses, excuses, luxe, lu...  \n",
       "2  [lively, messi, enzo, nolwenn, diaz, kourtney,...  \n",
       "3  [adams, refaeli, paltrow, brodier, brodier, ar...  \n",
       "4  [service, siege, avenue, douleur, a, la, poitr...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# tokenize\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "data['token'] = data.text_clean.apply(tokenizer.tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498191 words total, with a vocabulary size of 13159\n",
      "Max sentence length is 2570\n"
     ]
    }
   ],
   "source": [
    "# token check \n",
    "\n",
    "all_words = [word for tokens in data[\"token\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in data[\"token\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Model 1: Gender Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def tokenize(text):\n",
    "    # replace all non-alphabets and non-numbers with blank space\n",
    "    # text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    \n",
    "    # Tokenize words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # instantiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # instantiate stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "#     french stemmer \n",
    "    fr = SnowballStemmer('french')\n",
    "#     tags = nltk.pos_tag(tokens)\n",
    "    \n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        \n",
    "        \n",
    "        # lemmtize token using noun as part of speech\n",
    "        clean_tok = lemmatizer.lemmatize(tok)\n",
    "        # lemmtize token using verb as part of speech\n",
    "        clean_tok = lemmatizer.lemmatize(clean_tok, pos='v') #lclean_tok = lemmatizer.lemmatize(clean_tok, pos='v')\n",
    "        # stem token\n",
    "        clean_tok = stemmer.stem(clean_tok)\n",
    "        \n",
    "        # strip whitespace and append clean token to array\n",
    "        clean_tokens.append(clean_tok.strip())\n",
    "        \n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "data.text = data.text.astype('str')\n",
    "\n",
    "def cv(data):\n",
    "#     count_vectorizer = CountVectorizer(max_features=5000)\n",
    "    count_vectorizer = TfidfVectorizer(tokenizer=tokenize)\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "list_corpus = data.text_clean.tolist()\n",
    "list_labels = data[\"sex\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1340x10181 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 128098 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts.toarray(), y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_counts.toarray(), y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.750, precision = 0.754, recall = 0.750, f1 = 0.751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'tops': [(0.6747242411202897, 'universit'),\n",
       "   (0.6769224308852747, 'actu'),\n",
       "   (0.6787558137847918, 'cinema'),\n",
       "   (0.6970696110440102, 'ordonn'),\n",
       "   (0.7048707948286641, 'marin'),\n",
       "   (0.8013925538693795, 'franc'),\n",
       "   (0.8199407404933688, 'footbal'),\n",
       "   (0.8889909678094013, 'imag'),\n",
       "   (0.9847159680973916, 'sortir'),\n",
       "   (1.2468510160648678, 'actualit')],\n",
       "  'bottom': [(-0.7278595343553791, 'maeli'),\n",
       "   (-0.7795520029998417, 'even'),\n",
       "   (-0.7937661296612517, 'end'),\n",
       "   (-0.8152869337502288, 'mode'),\n",
       "   (-0.8379698813062801, 'automn'),\n",
       "   (-0.8530139930515642, 'week'),\n",
       "   (-0.8553432826717922, 'an'),\n",
       "   (-0.8636423132910206, 'gastronomi'),\n",
       "   (-0.9337977477575684, 'recett'),\n",
       "   (-1.1456612161704065, 'peopl')]}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n",
    "importance = get_most_important_features(count_vectorizer, clf, 10)\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.732, precision = 0.742, recall = 0.732, f1 = 0.733\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', C=0.5)\n",
    "clf.fit(X_train_counts.toarray(), y_train)\n",
    "y_predicted_counts = clf.predict(X_test_counts.toarray())\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.744, precision = 0.744, recall = 0.744, f1 = 0.744\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=10)\n",
    "clf.fit(X_train_counts.toarray(), y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts.toarray())\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Age Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "list_corpus = data.text_clean.tolist()\n",
    "list_labels = data[\"age\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "gbdt = GradientBoostingRegressor()\n",
    "gbdt.fit(X_train_counts, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165.0828924207356"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "mse = mean_squared_error(y_test, gbdt.predict(X_test_counts))\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042400533400846485"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, gbdt.predict(X_test_counts))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
